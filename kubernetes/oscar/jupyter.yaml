apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jupyter-claim
  namespace: science
spec:
  storageClassName: local-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  selector:
    matchLabels:
      type: "local"
---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jupyter-models-claim
  namespace: science
spec:
  storageClassName: local-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyter-deployment
  namespace: science
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyter-app
  template:
    metadata:
      labels:
        app: jupyter-app
    spec:
      # Request we be scheduled on a node with a hard disk type
      # In my setup, this is only the control-plane node
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: has_gpu
                operator: In
                values:
                  - "true"
      volumes:
        - name: bin
          hostPath:
            path: /usr/lib/nvidia-cuda-toolkit/bin
        - name: lib
          hostPath:
            path: /usr/lib/nvidia-cuda-toolkit/
        - name: libcuda-so-1
          hostPath:
            path: /usr/lib/x86_64-linux-gnu/libcuda.so.1
        - name: libcuda-so
          hostPath:
            path: /usr/lib/x86_64-linux-gnu/libcuda.so
        - name: jupyter-storage
          persistentVolumeClaim:
            claimName: jupyter-claim
        - name: jupyter-models-storage
          persistentVolumeClaim:
            claimName: jupyter-models-claim
      dnsPolicy: ClusterFirst
      containers:
        - name: jupyter-container
          image: robrohan/juypter-tensorflow-coral
          env:
            - name: TRANSFORMERS_CACHE
              value: "/data"
            - name: PASSWORD
              value: "password"
            - name: JUPYTER_TOKEN
              value: "easy"
          ports:
            - containerPort: 8888
              name: "http-server"
          volumeMounts:
            - mountPath: "/home/jovyan/work"
              name: jupyter-storage
            - mountPath: "/data"
              name: jupyter-models-storage
            - mountPath: /usr/local/nvidia/bin
              name: bin
            - mountPath: /usr/local/nvidia/lib
              name: lib
            - mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.1
              name: libcuda-so-1
            - mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so
              name: libcuda-so
          ### WARNING, this is only here because I am tool lazy to figure out the
          ### right way to get the coral libs working. If not using a coral TPU, 
          ### probably remove these.
          # securityContext:
          #   privileged: true
          #   allowPrivilegeEscalation: true
---

kind: Service
apiVersion: v1
metadata:
  name: jupyter-service
  namespace: science
spec:
  # We don't have a load balancer, so we need to specify the node IP
  # see nginx below...
  type: LoadBalancer
  selector:
    app: jupyter-app
  ports:
    - name: jupyter-http-default
      protocol: TCP
      port: 80              # port exposed by ingress / nginx
      targetPort: 8888      # port exposed by the pod
  externalTrafficPolicy: Local

#---
#
# apiVersion: networking.k8s.io/v1
# kind: Ingress
# metadata:
#   name: jupyter-ingress
#   namespace: science
# spec:
#   ingressClassName: nginx
#   tls:
#   rules:
#     # You'll need to edit your hosts file to point this domain to your cluster
#     # 192.168.1.15  example.lab.local
#     # for example. Because nginx uses the hostname to route the request and
#     # above we bound the service to the .15 address
#   - host: jupyter.lab.local
#     http:
#       paths:
#       - path: /
#         pathType: Prefix
#         backend:
#           service:
#             name: jupyter-service
#             port:
#               number: 80
